{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24bf2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Manual Output: 0.9999999999999059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training XOR NN: 100%|██████████| 20000/20000 [00:00<00:00, 22847.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XOR Trained Output [0, 0]: 0.009033699537611711\n",
      "XOR Trained Output [0, 1]: 0.9923292625479107\n",
      "XOR Trained Output [1, 0]: 0.9923280275107558\n",
      "XOR Trained Output [1, 1]: 0.007855695468228051\n",
      "\n",
      "Network Learned Weights:\n",
      " [[[6.953505610104288, 6.952785792366961, -3.148476196504664], [5.115899442661919, 5.115407875835948, -7.839603434415658]], [[10.961705832630566, -11.63060534664317, -5.144229056613083]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    # For simplicity, we include the bias term in weights\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "\n",
    "def dot(v, w):\n",
    "    # Calculates the dot product of the two input vectors\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    # Feed the input vector through the NN,\n",
    "    # and return the outputs of all layers separately\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        # Add a constant (bias), we use 1 for simplicity\n",
    "        input_with_bias = input_vector + [1]\n",
    "        # Calculate the output for each neuron in the layer\n",
    "        output = [neuron_output(neuron, input_with_bias)\n",
    "                  for neuron in layer]\n",
    "        # Add output results\n",
    "        outputs.append(output)\n",
    "\n",
    "        # Assign the current layer output as the next layers input\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "xor_network = [  # hidden layer\n",
    "    [[20.0, 20.0, -30.0],      # 'and' neuron\n",
    "     [20.0, 20.0, -10.0]],     # 'or'  neuron\n",
    "    # output layer\n",
    "    [[-60.0, 60.0, -30.0]]]    # '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "print(\"XOR Manual Output:\", feed_forward(xor_network, [0, 1])[-1][0])\n",
    "\n",
    "\n",
    "def sqerror_gradients(network, input_vector, target_vector):\n",
    "    # Using an input NN, input vector and target (output) vector\n",
    "    # First make a prediction,\n",
    "    # Then compute the gradient of the squared error loss\n",
    "    # (with respect to the neuron weights)\n",
    "\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                     dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]\n",
    "\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    # Multiply every element of vector v by constant c\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "\n",
    "def add(v, w):\n",
    "    # Add corresponding elements of two vectors\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    # Moves by step_size in the gradient direction from vector v\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    random.seed(0)\n",
    "\n",
    "    # training data\n",
    "    xs = [[0.0, 0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "    ys = [[0.0], [1.0], [1.0], [0.0]]\n",
    "\n",
    "    # start with random weights\n",
    "    network = [  # hidden layer: 2 inputs -> 2 outputs\n",
    "        [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "         [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "        # output layer: 2 inputs -> 1 output\n",
    "        [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "    ]\n",
    "\n",
    "    import tqdm\n",
    "\n",
    "    # Our models learning rate (i.e. gradient step increment)\n",
    "    learning_rate = 1.0\n",
    "\n",
    "    # For each epoch in range(20000)\n",
    "    for epoch in tqdm.trange(20000, desc=\"Training XOR NN\"):\n",
    "        # for each learning input, target pair\n",
    "        for x, y in zip(xs, ys):\n",
    "            # Calculate the network gradients\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                       for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "    # check that it learned XOR\n",
    "    print(\"\\nXOR Trained Output [0, 0]:\", feed_forward(network, [0, 0])[-1][0])\n",
    "    print(\"XOR Trained Output [0, 1]:\", feed_forward(network, [0, 1])[-1][0])\n",
    "    print(\"XOR Trained Output [1, 0]:\", feed_forward(network, [1, 0])[-1][0])\n",
    "    print(\"XOR Trained Output [1, 1]:\", feed_forward(network, [1, 1])[-1][0])\n",
    "\n",
    "    # Print the network weights\n",
    "    print(\"\\nNetwork Learned Weights:\\n\", network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21561ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
